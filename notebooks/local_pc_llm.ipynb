{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39bc3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc2c293f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2bfc5",
   "metadata": {},
   "source": [
    "# 1. The New Way - Docker Deploy\n",
    "\n",
    "To start, ensure you have Docker installed and running!\n",
    "\n",
    "In your terminal, run:\n",
    "```\n",
    "docker desktop enable model-runner --tcp=12434\n",
    "```\n",
    "\n",
    "Then run \n",
    "```\n",
    "docker model pull ai/ai/deepseek-r1-distill-llama:8B-Q4_K_M\n",
    "```\n",
    "to pull a quantized deepseek-r1 distilled llama 8b model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c004b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "docker_url = \"http://localhost:12434/engines/v1\"\n",
    "docker_llm = \"ai/deepseek-r1-distill-llama:8B-Q4_K_M\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5c59ae",
   "metadata": {},
   "source": [
    "#### Testing the Docker deployed model - with OpenAI!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5438caa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "docker_client = OpenAI(base_url=docker_url, api_key=\"fake\")\n",
    "\n",
    "prompt = \"Why is the sky blue?\"\n",
    "response = docker_client.chat.completions.create(\n",
    "    model=docker_llm,\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae713c9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sky appears blue primarily due to Rayleigh scattering, where shorter wavelengths of light, such as blue, are scattered more efficiently by gas molecules in the atmosphere. Here's a concise explanation:\n",
       "\n",
       "1. **Rayleigh Scattering**: This process causes light with shorter wavelengths (blue) to scatter more than longer wavelengths (red, orange). The scattering intensity is inversely proportional to the square of the wavelength, meaning blue light is scattered more.\n",
       "\n",
       "2. **Scattered Light**: The scattered blue light comes from all directions, but we only perceive the light that scatters towards us, creating the blue appearance of the sky.\n",
       "\n",
       "3. **Sunrise and Sunset**: During these times, the sun's light travels through more of the atmosphere, scattering away more blue light, allowing longer wavelengths (red and orange) to dominate the sky's appearance.\n",
       "\n",
       "4. **Human Perception**: Our eyes are more sensitive to blue, making it the most noticeable color in the sky.\n",
       "\n",
       "5. **Altitude and Atmosphere**: At higher altitudes, less atmosphere means less scattering, potentially making the sky appear black. On other planets, similar atmospheric conditions can result in blue skies, though other factors like atmospheric composition may alter the color.\n",
       "\n",
       "In summary, Rayleigh scattering is the primary reason for the blue sky during the day, with other factors influencing color changes like sunrise or sunset."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd87b9b",
   "metadata": {},
   "source": [
    "#### Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "16387e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "langchain_llm = ChatOpenAI(\n",
    "    model_name = docker_llm,\n",
    "    temperature = 0,\n",
    "    api_key = \"fake\",\n",
    "    openai_api_base = docker_url\n",
    ")\n",
    "\n",
    "response = langchain_llm.invoke([{\"role\": \"user\", \"content\": prompt}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bad51cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The sky appears blue due to a phenomenon known as Rayleigh scattering. Here's a concise explanation of the process:\n",
       "\n",
       "1. **Rayleigh Scattering**: This occurs when light scatters off small particles in the Earth's atmosphere. The scattering is more efficient for shorter wavelengths of light, such as blue and violet.\n",
       "\n",
       "2. **Forward Scattering**: The scattered light is directed in a manner that is close to the original direction of travel. This means that the scattered blue light we see is coming towards us, contributing to the blue appearance of the sky.\n",
       "\n",
       "3. **Atmospheric Density**: The Earth's atmosphere is denser near the surface, leading to more scattering of blue light. This results in the sky appearing blue to observers on the ground.\n",
       "\n",
       "4. **Sunrise and Sunset**: During these times, the sun's light passes through a greater thickness of the atmosphere, scattering away blue and violet light, allowing red light to dominate and giving the sky its red hue.\n",
       "\n",
       "5. **Black Sky in Space**: In the absence of an atmosphere, as experienced by astronauts, the sky appears black because there is no scattering of light.\n",
       "\n",
       "In summary, the blue color of the sky is primarily due to Rayleigh scattering of blue light, which is efficiently scattered and directed towards us, combined with the Earth's atmospheric density near the surface."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668caedc",
   "metadata": {},
   "source": [
    "#### Using LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7566f26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hi! How can I assist you today?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "llm = OpenAILike(\n",
    "    model = docker_llm,\n",
    "    api_base = docker_url,\n",
    "    api_key = \"fake\",\n",
    "    is_chat_model = True,\n",
    "    is_function_calling_model = True,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "response = llm.complete(\"Hi!\")\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcec6e4",
   "metadata": {},
   "source": [
    "Creating a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "630220e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent.workflow import AgentStream, FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Use this tool to get the weather in a location.\"\"\"\n",
    "    return f\"The weather in {location} is sunny.\"\n",
    "\n",
    "weather_agent = FunctionAgent(\n",
    "    llm = llm,\n",
    "    tools = [FunctionTool.from_defaults(get_weather)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76f3910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the weather in Singapore, you can use the `get_weather` tool by calling it with the location \"Singapore\". Here's how you can do it:\n",
      "\n",
      "```json\n",
      "<tool call=\"get_weather\" location=\"Singapore\">\n",
      "  {\"weather\": \"Sunny with a temperature of 28Â°C.\"}\n",
      "</tool call>\n",
      "```\n",
      "\n",
      "This tool call will return the current weather in Singapore, such as the temperature and whether it's sunny or not."
     ]
    }
   ],
   "source": [
    "handler = weather_agent.run(\"What is the weather in Singapore?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentStream):\n",
    "        print(event.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1fd7f1",
   "metadata": {},
   "source": [
    "Deploy LlamaIndex Workflow as an MCP!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3552c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ../tools/weather_mcp.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ../tools/weather_mcp.py\n",
    "\n",
    "\"\"\"\n",
    "This code will automatically generate a FastMCP server that will\n",
    "\n",
    "- Use the agent workflow class name as the tool name\n",
    "- Use our custom RunEvent as the typed inputs to the tool\n",
    "- Automatically use the SSE stream for streaming json dumps of the workflow event stream\n",
    "\"\"\"\n",
    "\n",
    "from llama_index.core.agent.workflow import AgentStream, FunctionAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "from llama_index.llms.openai_like import OpenAILike\n",
    "from llama_index.tools.mcp.utils import workflow_as_mcp\n",
    "\n",
    "\n",
    "docker_url = \"http://localhost:12434/engines/v1\"\n",
    "docker_llm = \"ai/deepseek-r1-distill-llama:8B-Q4_K_M\"\n",
    "\n",
    "llm = OpenAILike(\n",
    "    model = docker_llm,\n",
    "    api_base = docker_url,\n",
    "    api_key = \"fake\",\n",
    "    is_chat_model = True,\n",
    "    is_function_calling_model = True,\n",
    "    temperature = 0\n",
    ")\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Use this tool to get the weather in a location.\"\"\"\n",
    "    return f\"The weather in {location} is sunny.\"\n",
    "\n",
    "weather_agent = FunctionAgent(\n",
    "    llm = llm,\n",
    "    name = \"weather_agent\",\n",
    "    description = \"An agent that can answer questions about the weather.\",\n",
    "    tools = [FunctionTool.from_defaults(get_weather)]\n",
    ")\n",
    "\n",
    "mcp = workflow_as_mcp(weather_agent, workflow_name = 'weather_agent', port=8000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport=\"sse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f581810",
   "metadata": {},
   "source": [
    "Spin up the mcp server:\n",
    "`uv run tools/weather/mcp.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3be62d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<llama_index.core.tools.function_tool.FunctionTool object at 0x135995040>]\n"
     ]
    }
   ],
   "source": [
    "# Test if tools have been deployed!\n",
    "\n",
    "from llama_index.tools.mcp import BasicMCPClient, McpToolSpec\n",
    "\n",
    "mcp_client = BasicMCPClient(\"http://localhost:8000/sse\")\n",
    "mcp_tool = McpToolSpec(client=mcp_client)\n",
    "tools = await mcp_tool.to_tool_list_async()\n",
    "print(tools)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162c570c",
   "metadata": {},
   "source": [
    "Testing with OpenAI-Like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d56d9c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To find the current weather in Singapore, you can use the weather_agent tool by calling it with the location parameter set to \"Singapore\". Here's how to do it:\n",
      "\n",
      "```json\n",
      "<tool call begin>weather_agent<tool call end> run_args=\"Singapore\"\n",
      "```"
     ]
    }
   ],
   "source": [
    "agent = FunctionAgent(\n",
    "    llm = llm,\n",
    "    tools = tools\n",
    ")\n",
    "handler = agent.run(\"What is the weather in Singapore?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentStream):\n",
    "        print(event.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09674fa",
   "metadata": {},
   "source": [
    "Testing with OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2e8841fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompletionResponse(text='Hello! How can I assist you today?', additional_kwargs={'prompt_tokens': 9, 'completion_tokens': 9, 'total_tokens': 18}, raw=ChatCompletion(id='chatcmpl-BtyBk9LRwyrH16KnC4KgLrnM3ytps', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1752679104, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=9, prompt_tokens=9, total_tokens=18, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))), logprobs=None, delta=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai_llm = OpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "openai_llm.complete(\"Hi!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d03f956e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm currently unable to retrieve the weather information for Singapore. You might want to check a weather website or app for the latest updates. If you have any other questions or need assistance, feel free to ask!"
     ]
    }
   ],
   "source": [
    "agent = FunctionAgent(\n",
    "    llm = openai_llm,\n",
    "    tools = tools\n",
    ")\n",
    "handler = agent.run(\"What is the weather in Singapore?\")\n",
    "\n",
    "async for event in handler.stream_events():\n",
    "    if isinstance(event, AgentStream):\n",
    "        print(event.delta, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60009855",
   "metadata": {},
   "source": [
    "# 2. My favorite way - Ollama!\n",
    "\n",
    "To start, ensure that you have Ollama installed and running.\n",
    "\n",
    "In your terminal, run:\n",
    "```\n",
    "ollama pull qwen2.5\n",
    "```\n",
    "\n",
    "This will pull qwen2.5:7b by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acf84afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_url = \"http://localhost:11434/v1/\"\n",
    "ollama_llm = \"qwen3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af83060",
   "metadata": {},
   "source": [
    "#### Using the OpenAI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d27a28c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user is asking why the sky is blue. I remember that this is a classic question related to the scattering of sunlight. Let me think about the process step by step. \n",
       "\n",
       "First, sunlight is white, but it's actually a combination of all colors. Each color has a different wavelength. When sunlight enters Earth's atmosphere, it interacts with the molecules and small particles in the air. I think Rayleigh scattering is involved here. Rayleigh scattering refers to the scattering of light by particles much smaller than the wavelength of the light. \n",
       "\n",
       "So, shorter wavelengths (like blue and violet) are scattered more than longer wavelengths (like red and orange). That means blue light is scattered in all directions by the molecules in the atmosphere. But wait, why isn't the sky violet then? Because even though violet light has an even shorter wavelength and is scattered more, the sun emits less violet light than blue. Also, our eyes are more sensitive to blue light, so we perceive the sky as blue rather than violet.\n",
       "\n",
       "But I should also mention the role of the atmosphere's composition. Nitrogen and oxygen molecules are the main contributors to this scattering. The presence of other particles, like dust or water vapor, can affect the color, but under normal conditions, the blue dominates. \n",
       "\n",
       "Additionally, during sunrise or sunset, the sky appears reddish because the sunlight has to pass through more atmosphere, scattering out the shorter wavelengths and leaving the longer ones. That's why the sun looks red or orange during those times.\n",
       "\n",
       "Wait, but I should make sure I'm not confusing Rayleigh scattering with Mie scattering. Mie scattering is for larger particles, like water droplets or aerosols, which cause more uniform scattering and can lead to white or pale colors. That's why clouds appear white, as they scatter all wavelengths of light equally.\n",
       "\n",
       "So putting it all together, the primary reason the sky is blue is due to Rayleigh scattering of shorter wavelengths like blue and violet, combined with our eyes' sensitivity to blue light and the sun's emission spectrum. Also, the fact that violet is scattered even more but is less intense and our eyes are less sensitive to it explains why the sky isn't violet. \n",
       "\n",
       "I should also address possible misconceptions, like thinking the sky is blue because of reflection from the Earth's surface or something else. No, that's not the main reason. The main cause is indeed the scattering of sunlight by atmospheric molecules.\n",
       "</think>\n",
       "\n",
       "The sky appears blue due to a phenomenon called **Rayleigh scattering**, which involves the interaction of sunlight with the molecules and small particles in Earth's atmosphere. Here's a concise explanation:\n",
       "\n",
       "### 1. **Sunlight and Color**:\n",
       "   - Sunlight is composed of a spectrum of colors, each with different wavelengths (from violet/blue to red/orange).\n",
       "   - Shorter wavelengths (like **blue** and **violet**) are scattered more efficiently by the atmosphere than longer wavelengths (like red or yellow).\n",
       "\n",
       "### 2. **Rayleigh Scattering**:\n",
       "   - When sunlight enters the atmosphere, it interacts with molecules like **oxygen (Oâ)** and **nitrogen (Nâ)**. These molecules are much smaller than the wavelength of visible light.\n",
       "   - Blue/violet light (shorter wavelengths) is scattered in all directions by these molecules, while red/orange light (longer wavelengths) passes through more directly.\n",
       "\n",
       "### 3. **Why the Sky Appears Blue, Not Violet**:\n",
       "   - Although **violet** light is scattered even more than blue, the **sun emits less violet light** than blue.\n",
       "   - Human eyes are **more sensitive to blue light** than violet, and the **violet light is partially absorbed by the upper atmosphere**.\n",
       "   - Together, these factors make blue the dominant color we perceive.\n",
       "\n",
       "### 4. **Additional Factors**:\n",
       "   - During **sunrise or sunset**, sunlight travels through more atmosphere, scattering out the blue light and leaving the longer wavelengths (red/orange) to dominate.\n",
       "   - **Clouds** and larger particles cause **Mie scattering**, which scatters all wavelengths equally, giving clouds their white or gray appearance.\n",
       "\n",
       "### Summary:\n",
       "The blue sky is a result of **Rayleigh scattering** of shorter wavelengths (blue/violet) by atmospheric molecules, combined with our eyes' sensitivity to blue light and the sun's emission spectrum. This makes blue the dominant color we see on clear days! ð¤ï¸"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ollama_client = OpenAI(base_url=ollama_url, api_key=\"fake\")\n",
    "\n",
    "response = ollama_client.chat.completions.create(\n",
    "    model=ollama_llm,\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")\n",
    "\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ea8df",
   "metadata": {},
   "source": [
    "#### Using the Ollama SDK\n",
    "\n",
    "Which looks like the OpenAI SDK!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59df9ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user is asking why the sky is blue. I remember from school that it has to do with light scattering, but I need to recall the exact details. Let me start by explaining that sunlight is white but consists of different colors. Each color has a different wavelength.\n",
      "\n",
      "Then there's Rayleigh scattering. I think that shorter wavelengths scatter more. Blue and violet have shorter wavelengths, so they scatter more. But why is the sky blue and not violet? Maybe because our eyes are more sensitive to blue, and the sun emits more blue light than violet. Also, some of the violet light is absorbed by the upper atmosphere.\n",
      "\n",
      "Wait, I should mention that during sunrise or sunset, the sky turns red or orange because the light has to travel through more atmosphere, scattering the shorter wavelengths out of our line of sight. That's an important point to include for a complete answer.\n",
      "\n",
      "Let me structure this step by step. Start with the composition of sunlight, then explain Rayleigh scattering, the role of blue and violet light, the sensitivity of human eyes, and the effect of atmospheric path length. Make sure to keep it simple and avoid jargon. Check if there's any confusion between Rayleigh and Mie scattering, but since the question is about the sky's color, Rayleigh is the main factor here. Also, maybe mention that the atmosphere's molecules (like nitrogen and oxygen) are the ones causing the scattering. Yeah, that's right. So putting it all together in a clear, concise way without getting too technical should work.\n",
      "</think>\n",
      "\n",
      "The sky appears blue due to a phenomenon called **Rayleigh scattering**, which involves how sunlight interacts with the Earth's atmosphere. Here's a simplified explanation:\n",
      "\n",
      "1. **Sunlight Composition**: Sunlight is a mix of all visible colors (red, orange, yellow, green, blue, indigo, violet), each with different wavelengths. White light is a combination of these colors.\n",
      "\n",
      "2. **Scattering of Light**: When sunlight enters Earth's atmosphere, it interacts with gas molecules (like nitrogen and oxygen) and small particles. Shorter wavelengths (blue and violet) scatter more easily than longer wavelengths (red, orange, etc.). This is because the scattering intensity is inversely proportional to the fourth power of the wavelength (Rayleigh scattering).\n",
      "\n",
      "3. **Why Blue, Not Violet?**:\n",
      "   - **Blue light** (around 450â495 nm) scatters more than violet (around 380â450 nm), but both are scattered.\n",
      "   - The **solar spectrum** emits more blue light than violet, and our eyes are more sensitive to blue.\n",
      "   - Some **violet light** is absorbed by the upper atmosphere, and our eyes perceive it less strongly.\n",
      "\n",
      "4. **Result**: The scattered blue light fills the sky, making it appear blue to us. During sunrise or sunset, the sunlight travels through more atmosphere, scattering out the shorter wavelengths (blue/violet) and leaving the longer wavelengths (red/orange) to dominate, creating the reddish hues.\n",
      "\n",
      "In summary, the blue sky is a result of **shorter wavelengths (blue light) scattering more efficiently** in the atmosphere, combined with our eyes' sensitivity to blue and the sun's emission spectrum. ð\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(\n",
    "    model=ollama_llm, \n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "  )\n",
    "\n",
    "print(response['message']['content'])\n",
    "# or access fields directly from the response object\n",
    "# print(response.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc81456",
   "metadata": {},
   "source": [
    "#### Using LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eec49600",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(\n",
    "    model = ollama_llm,\n",
    "    temperature = 0\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    (\n",
    "        \"system\",\n",
    "        \"You are a helpful assistant that translates English to Chinese. Translate the user sentence.\",\n",
    "    ),\n",
    "    (\"human\", \"I love programming.\"),\n",
    "]\n",
    "ai_msg = model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "baa8096e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<think>\n",
       "Okay, the user wants me to translate \"I love programming.\" from English to Chinese. Let me start by understanding the sentence. The main components are \"I\" (subject), \"love\" (verb), and \"programming\" (object). \n",
       "\n",
       "First, \"I\" in Chinese is \"æ\". Then \"love\" is a verb, and the translation for \"love\" in this context would be \"ç±\" or \"åæ¬¢\". Both are correct, but \"ç±\" is more intense, while \"åæ¬¢\" is a bit more casual. Since the user said \"love\", maybe \"ç±\" is better here.\n",
       "\n",
       "Next, \"programming\" translates to \"ç¼ç¨\". So putting it all together: \"æç±ç¼ç¨ã\" Alternatively, \"æåæ¬¢ç¼ç¨ã\" Both are correct. But the user might prefer the more direct translation. Let me check if there are any nuances. \n",
       "\n",
       "In Chinese, \"ç±\" is often used for deeper affection, while \"åæ¬¢\" is more about preference. Since the user says \"love\", using \"ç±\" would be more accurate. So the translation should be \"æç±ç¼ç¨ã\" \n",
       "\n",
       "Wait, but sometimes people might use \"åæ¬¢\" even when they mean \"love\" in a general sense. Maybe the user is okay with either. But since the instruction is to translate accurately, \"ç±\" is better. \n",
       "\n",
       "I should also check for any possible errors. The structure is correct: subject + verb + object. No particles needed. So the final translation is \"æç±ç¼ç¨ã\"\n",
       "</think>\n",
       "\n",
       "æç±ç¼ç¨ã"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(ai_msg.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4a2ef",
   "metadata": {},
   "source": [
    "Creating our MCP agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b2a3fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"weather\": {\n",
    "            # Make sure you start your weather server on port 8000\n",
    "            \"url\": \"http://localhost:8000/sse/\",\n",
    "            \"transport\": \"sse\",\n",
    "        }\n",
    "    }\n",
    ")\n",
    "tools = await client.get_tools()\n",
    "agent = create_react_agent(model, tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10aa3a4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='weather_agent', description='An agent that can answer questions about the weather.', args_schema={'$defs': {'AgentWorkflowStartEvent': {'properties': {}, 'title': 'AgentWorkflowStartEvent', 'type': 'object'}}, 'properties': {'run_args': {'$ref': '#/$defs/AgentWorkflowStartEvent'}}, 'required': ['run_args'], 'title': '_workflow_toolArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x13167f920>)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c1af05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': {'messages': [AIMessage(content='<think>\\nOkay, the user is asking for the weather in SF, which I assume stands for San Francisco. I need to use the weather_agent function to get this information. Let me check the function parameters. The function requires a run_args object, but the description for run_args is empty. Maybe the function expects certain parameters like location. Since the user mentioned SF, I should pass that as the location. Let me structure the arguments with \"location\": \"San Francisco\" to ensure the function knows where to check the weather. I\\'ll format the tool call accordingly.\\n</think>\\n\\n', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-07-16T14:54:22.064369Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2815476958, 'load_duration': 33805833, 'prompt_eval_count': 148, 'prompt_eval_duration': 178164875, 'eval_count': 142, 'eval_duration': 2603170333, 'model_name': 'qwen3'}, id='run--31c2f206-443d-41fd-bb7a-4b4929b3d2a3-0', tool_calls=[{'name': 'weather_agent', 'args': {'run_args': {'location': 'San Francisco'}}, 'id': 'a254fc94-842d-48ba-be9e-df4392423afe', 'type': 'tool_call'}], usage_metadata={'input_tokens': 148, 'output_tokens': 142, 'total_tokens': 290})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Error: ToolException(\"Error executing tool weather_agent: Error in step \\'init_run\\': Must provide either user_msg or chat_history\")\\n Please fix your mistakes.', name='weather_agent', id='a037a949-35a1-4957-b3b9-0ac79923ca10', tool_call_id='a254fc94-842d-48ba-be9e-df4392423afe', status='error')]}}\n",
      "{'agent': {'messages': [AIMessage(content='<think>\\nOkay, let me try to figure out what went wrong here. The user asked for the weather in SF, and I tried to use the weather_agent function. But there was an error saying that the tool requires either user_msg or chat_history. Hmm, the function\\'s parameters mentioned run_args, but maybe the agent expects those arguments to include the user\\'s message or chat history.\\n\\nWait, the initial function definition didn\\'t specify the parameters properly. The run_args object was described as empty, but maybe the actual implementation requires certain fields. The error message suggests that the tool needs either user_msg or chat_history. Since the user\\'s query was \"what is the weather in sf\", perhaps I should pass that as the user_msg in the run_args.\\n\\nSo, I should adjust the function call to include \"user_msg\": \"what is the weather in sf\" within the run_args. That way, the tool can process the user\\'s request correctly. Let me try that again.\\n</think>\\n\\n', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-07-16T14:54:27.203524Z', 'done': True, 'done_reason': 'stop', 'total_duration': 4914331042, 'load_duration': 40435083, 'prompt_eval_count': 309, 'prompt_eval_duration': 117883625, 'eval_count': 231, 'eval_duration': 4753420917, 'model_name': 'qwen3'}, id='run--f7555439-f246-445d-8fb4-f648ae6a6f05-0', tool_calls=[{'name': 'weather_agent', 'args': {'run_args': {'user_msg': 'what is the weather in sf'}}, 'id': '0dfd5f80-3448-45cf-9765-22188ad24c61', 'type': 'tool_call'}], usage_metadata={'input_tokens': 309, 'output_tokens': 231, 'total_tokens': 540})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Error: ToolException(\"Error executing tool weather_agent: Error in step \\'init_run\\': Must provide either user_msg or chat_history\")\\n Please fix your mistakes.', name='weather_agent', id='b3dbbf1b-1610-4e47-a862-d116e1d1ca4e', tool_call_id='0dfd5f80-3448-45cf-9765-22188ad24c61', status='error')]}}\n",
      "{'agent': {'messages': [AIMessage(content=\"<think>\\nOkay, let me try to figure out what's going on here. The user asked for the weather in SF, and I tried to use the weather_agent function. But there's an error saying that the tool requires either user_msg or chat_history. Wait, the initial function definition didn't specify those parameters. Maybe the tool's actual parameters are different from what was described?\\n\\nThe first tool definition had run_args as an empty object, but the error suggests that user_msg or chat_history is needed. Maybe there was a mistake in the tool's parameter setup. Since the user's query is straightforward, perhaps I should just provide the location as a parameter. Let me check if the weather_agent function expects a location parameter. If the tool requires user_msg, maybe I should format the request as a message. Alternatively, maybe the tool's parameters were not properly defined, and I should adjust the arguments accordingly. Let me try passing the location in the run_args even though the parameters weren't specified.\\n</think>\\n\\n\", additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-07-16T14:54:32.394226Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5056871916, 'load_duration': 45235791, 'prompt_eval_count': 554, 'prompt_eval_duration': 120450792, 'eval_count': 229, 'eval_duration': 4883099500, 'model_name': 'qwen3'}, id='run--a9981da5-5939-4e6e-b857-dc7f1ec405fe-0', tool_calls=[{'name': 'weather_agent', 'args': {'run_args': {'location': 'San Francisco'}}, 'id': '85298e64-7693-46f6-bb26-36692777acc6', 'type': 'tool_call'}], usage_metadata={'input_tokens': 554, 'output_tokens': 229, 'total_tokens': 783})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Error: ToolException(\"Error executing tool weather_agent: Error in step \\'init_run\\': Must provide either user_msg or chat_history\")\\n Please fix your mistakes.', name='weather_agent', id='701a9848-f97b-498c-bd36-096cf2409104', tool_call_id='85298e64-7693-46f6-bb26-36692777acc6', status='error')]}}\n",
      "{'agent': {'messages': [AIMessage(content=\"<think>\\nOkay, I need to figure out why the tool is throwing an error. The user asked for the weather in SF, and I tried to call the weather_agent function. The error says that the tool requires either user_msg or chat_history. But the initial tool definition didn't mention those parameters. Maybe there was a mistake in how the tool was set up. Let me check the function's parameters again. The original tool had run_args as an empty object, but the error suggests that user_msg or chat_history is needed. Perhaps the tool's actual parameters are different, and the initial description was incomplete. Since the user's query is a simple request, maybe I should format the function call with user_msg as the user's question. Let me try that.\\n</think>\\n\\n\", additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-07-16T14:54:36.521321Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3977726417, 'load_duration': 46697542, 'prompt_eval_count': 802, 'prompt_eval_duration': 123876458, 'eval_count': 187, 'eval_duration': 3792311667, 'model_name': 'qwen3'}, id='run--4531b4c2-c812-41f7-a56f-c684cd8cc602-0', tool_calls=[{'name': 'weather_agent', 'args': {'run_args': {'user_msg': 'what is the weather in sf'}}, 'id': 'ddb15ab6-75be-4e77-8bfe-a22446868bbf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 802, 'output_tokens': 187, 'total_tokens': 989})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Error: ToolException(\"Error executing tool weather_agent: Error in step \\'init_run\\': Must provide either user_msg or chat_history\")\\n Please fix your mistakes.', name='weather_agent', id='0362b7aa-7a56-44b4-9dd5-c78a15b9250f', tool_call_id='ddb15ab6-75be-4e77-8bfe-a22446868bbf', status='error')]}}\n",
      "{'agent': {'messages': [AIMessage(content=\"<think>\\nOkay, I'm stuck here. The user is asking for the weather in SF, and every time I try to call the weather_agent function, I get the same error about needing user_msg or chat_history. But the tool's parameters were supposed to be run_args, which wasn't defined. Maybe the tool's parameters were misconfigured, and the correct parameter is something else like location. Let me try passing the location directly in the run_args even though the parameters weren't specified. Alternatively, maybe the tool expects a different structure. I'll make a guess and include the location in the arguments.\\n</think>\\n\\n\", additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-07-16T14:54:39.994186Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3323637500, 'load_duration': 44286708, 'prompt_eval_count': 1003, 'prompt_eval_duration': 126333709, 'eval_count': 150, 'eval_duration': 3130823708, 'model_name': 'qwen3'}, id='run--1c10545a-b414-4e7e-be8c-49fac1392ace-0', tool_calls=[{'name': 'weather_agent', 'args': {'run_args': {'location': 'San Francisco'}}, 'id': '84503fd2-05c7-4d47-9827-ac59115dc9f5', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1003, 'output_tokens': 150, 'total_tokens': 1153})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Error: ToolException(\"Error executing tool weather_agent: Error in step \\'init_run\\': Must provide either user_msg or chat_history\")\\n Please fix your mistakes.', name='weather_agent', id='66400a7b-0d8c-4d82-8098-1c8354577c19', tool_call_id='84503fd2-05c7-4d47-9827-ac59115dc9f5', status='error')]}}\n",
      "{'agent': {'messages': [AIMessage(content='<think>\\nOkay, I need to resolve this error. The user is asking for the weather in SF, and the tool is throwing an error about needing user_msg or chat_history. But the tool\\'s parameters were supposed to be run_args. Maybe the tool\\'s actual parameters are different from what was described. Let me check if the weather_agent function requires user_msg or chat_history. If so, I should format the function call with those parameters. Since the user\\'s query is \"what is the weather in sf\", I\\'ll use that as the user_msg. Let me try that.\\n</think>\\n\\n', additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-07-16T14:54:43.552545Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3415091042, 'load_duration': 49310875, 'prompt_eval_count': 1172, 'prompt_eval_duration': 125375375, 'eval_count': 150, 'eval_duration': 3210263958, 'model_name': 'qwen3'}, id='run--a3491dfd-0945-458d-88b4-5d47c53d2ece-0', tool_calls=[{'name': 'weather_agent', 'args': {'run_args': {'user_msg': 'what is the weather in sf'}}, 'id': '2c7e65bf-f07b-4020-9110-80dd067214c1', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1172, 'output_tokens': 150, 'total_tokens': 1322})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Error: ToolException(\"Error executing tool weather_agent: Error in step \\'init_run\\': Must provide either user_msg or chat_history\")\\n Please fix your mistakes.', name='weather_agent', id='b0863db5-dd1a-4411-a694-bcf568aa1501', tool_call_id='2c7e65bf-f07b-4020-9110-80dd067214c1', status='error')]}}\n",
      "{'agent': {'messages': [AIMessage(content=\"<think>\\nOkay, I'm really confused now. The user is asking for the weather in SF, and every time I try to call the weather_agent function, I get the same error about needing user_msg or chat_history. But the tool's parameters were supposed to be run_args, not user_msg. Maybe there was a mistake in the tool's setup, and the correct parameter is something else. Let me try passing the location as a parameter in run_args, even though the parameters weren't defined. Alternatively, maybe the tool expects a different structure. I'll make a guess and include the location in the arguments.\\n</think>\\n\\n\", additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-07-16T14:54:47.264986Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3523344417, 'load_duration': 45529958, 'prompt_eval_count': 1336, 'prompt_eval_duration': 126957583, 'eval_count': 152, 'eval_duration': 3318751250, 'model_name': 'qwen3'}, id='run--b5178b20-10fa-472a-9d8d-68f1433db0c2-0', tool_calls=[{'name': 'weather_agent', 'args': {'run_args': {'location': 'San Francisco'}}, 'id': '2b4e66fc-d0ac-4193-985c-87c2065b3e35', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1336, 'output_tokens': 152, 'total_tokens': 1488})]}}\n",
      "{'tools': {'messages': [ToolMessage(content='Error: ToolException(\"Error executing tool weather_agent: Error in step \\'init_run\\': Must provide either user_msg or chat_history\")\\n Please fix your mistakes.', name='weather_agent', id='06c813d8-1247-44e0-9398-6c82a1705a5a', tool_call_id='2b4e66fc-d0ac-4193-985c-87c2065b3e35', status='error')]}}\n"
     ]
    },
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m agent.astream(\n\u001b[32m      2\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mwhat is the weather in sf\u001b[39m\u001b[33m\"\u001b[39m}]},\n\u001b[32m      3\u001b[39m     stream_mode=\u001b[33m\"\u001b[39m\u001b[33mupdates\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m ):\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# chunk['messages'].pretty_print()\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(chunk)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langgraph/pregel/__init__.py:2768\u001b[39m, in \u001b[36mPregel.astream\u001b[39m\u001b[34m(self, input, config, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, checkpoint_during, debug, subgraphs)\u001b[39m\n\u001b[32m   2766\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m loop.amatch_cached_writes():\n\u001b[32m   2767\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2768\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner.atick(\n\u001b[32m   2769\u001b[39m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop.tasks.values() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t.writes],\n\u001b[32m   2770\u001b[39m     timeout=\u001b[38;5;28mself\u001b[39m.step_timeout,\n\u001b[32m   2771\u001b[39m     get_waiter=get_waiter,\n\u001b[32m   2772\u001b[39m     schedule_task=loop.aaccept_push,\n\u001b[32m   2773\u001b[39m ):\n\u001b[32m   2774\u001b[39m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[32m   2775\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m _output(\n\u001b[32m   2776\u001b[39m         stream_mode,\n\u001b[32m   2777\u001b[39m         print_mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2780\u001b[39m         asyncio.QueueEmpty,\n\u001b[32m   2781\u001b[39m     ):\n\u001b[32m   2782\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m o\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langgraph/pregel/runner.py:295\u001b[39m, in \u001b[36mPregelRunner.atick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    293\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m295\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m arun_with_retry(\n\u001b[32m    296\u001b[39m         t,\n\u001b[32m    297\u001b[39m         retry_policy,\n\u001b[32m    298\u001b[39m         stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    299\u001b[39m         configurable={\n\u001b[32m    300\u001b[39m             CONFIG_KEY_CALL: partial(\n\u001b[32m    301\u001b[39m                 _acall,\n\u001b[32m    302\u001b[39m                 weakref.ref(t),\n\u001b[32m    303\u001b[39m                 stream=\u001b[38;5;28mself\u001b[39m.use_astream,\n\u001b[32m    304\u001b[39m                 retry_policy=retry_policy,\n\u001b[32m    305\u001b[39m                 futures=weakref.ref(futures),\n\u001b[32m    306\u001b[39m                 schedule_task=schedule_task,\n\u001b[32m    307\u001b[39m                 submit=\u001b[38;5;28mself\u001b[39m.submit,\n\u001b[32m    308\u001b[39m                 loop=loop,\n\u001b[32m    309\u001b[39m             ),\n\u001b[32m    310\u001b[39m         },\n\u001b[32m    311\u001b[39m     )\n\u001b[32m    312\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langgraph/pregel/retry.py:137\u001b[39m, in \u001b[36marun_with_retry\u001b[39m\u001b[34m(task, retry_policy, stream, match_cached_writes, configurable)\u001b[39m\n\u001b[32m    135\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    136\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m task.proc.ainvoke(task.input, config)\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    139\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:672\u001b[39m, in \u001b[36mRunnableSeq.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    670\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    671\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(\n\u001b[32m    673\u001b[39m             step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs), context=context\n\u001b[32m    674\u001b[39m         )\n\u001b[32m    675\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    676\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[38;5;28;01mawait\u001b[39;00m step.ainvoke(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:198\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == _CANCELLED:\n\u001b[32m    197\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._make_cancelled_error()\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state != _FINISHED:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.InvalidStateError(\u001b[33m'\u001b[39m\u001b[33mResult is not ready.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langgraph/utils/runnable.py:431\u001b[39m, in \u001b[36mRunnableCallable.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    429\u001b[39m         run = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    430\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m         ret = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.create_task(coro, context=context)\n\u001b[32m    432\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    433\u001b[39m     ret = \u001b[38;5;28;01mawait\u001b[39;00m coro\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:198\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == _CANCELLED:\n\u001b[32m    197\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._make_cancelled_error()\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state != _FINISHED:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.InvalidStateError(\u001b[33m'\u001b[39m\u001b[33mResult is not ready.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langgraph/prebuilt/chat_agent_executor.py:525\u001b[39m, in \u001b[36mcreate_react_agent.<locals>.acall_model\u001b[39m\u001b[34m(state, config)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macall_model\u001b[39m(state: StateSchema, config: RunnableConfig) -> StateSchema:\n\u001b[32m    524\u001b[39m     state = _get_model_input_state(state)\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m     response = cast(AIMessage, \u001b[38;5;28;01mawait\u001b[39;00m model_runnable.ainvoke(state, config))\n\u001b[32m    526\u001b[39m     \u001b[38;5;66;03m# add agent name to the AIMessage\u001b[39;00m\n\u001b[32m    527\u001b[39m     response.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:3088\u001b[39m, in \u001b[36mRunnableSequence.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3086\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3087\u001b[39m                 part = functools.partial(step.ainvoke, input_, config)\n\u001b[32m-> \u001b[39m\u001b[32m3088\u001b[39m             input_ = \u001b[38;5;28;01mawait\u001b[39;00m coro_with_context(part(), context, create_task=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3089\u001b[39m     \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3090\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:198\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == _CANCELLED:\n\u001b[32m    197\u001b[39m     exc = \u001b[38;5;28mself\u001b[39m._make_cancelled_error()\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state != _FINISHED:\n\u001b[32m    200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions.InvalidStateError(\u001b[33m'\u001b[39m\u001b[33mResult is not ready.\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langchain_core/runnables/base.py:5447\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5440\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5441\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5442\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5445\u001b[39m     **kwargs: Optional[Any],\n\u001b[32m   5446\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5447\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5448\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5449\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5450\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5451\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:400\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    392\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    397\u001b[39m     **kwargs: Any,\n\u001b[32m    398\u001b[39m ) -> BaseMessage:\n\u001b[32m    399\u001b[39m     config = ensure_config(config)\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m    401\u001b[39m         [\u001b[38;5;28mself\u001b[39m._convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[32m    402\u001b[39m         stop=stop,\n\u001b[32m    403\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    404\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    405\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    406\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    407\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    408\u001b[39m         **kwargs,\n\u001b[32m    409\u001b[39m     )\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:974\u001b[39m, in \u001b[36mBaseChatModel.agenerate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m    965\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    966\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34magenerate_prompt\u001b[39m(\n\u001b[32m    967\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    971\u001b[39m     **kwargs: Any,\n\u001b[32m    972\u001b[39m ) -> LLMResult:\n\u001b[32m    973\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m--> \u001b[39m\u001b[32m974\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate(\n\u001b[32m    975\u001b[39m         prompt_messages, stop=stop, callbacks=callbacks, **kwargs\n\u001b[32m    976\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:894\u001b[39m, in \u001b[36mBaseChatModel.agenerate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    881\u001b[39m run_managers = \u001b[38;5;28;01mawait\u001b[39;00m callback_manager.on_chat_model_start(\n\u001b[32m    882\u001b[39m     \u001b[38;5;28mself\u001b[39m._serialized,\n\u001b[32m    883\u001b[39m     messages_to_trace,\n\u001b[32m   (...)\u001b[39m\u001b[32m    888\u001b[39m     run_id=run_id,\n\u001b[32m    889\u001b[39m )\n\u001b[32m    891\u001b[39m input_messages = [\n\u001b[32m    892\u001b[39m     _normalize_messages(message_list) \u001b[38;5;28;01mfor\u001b[39;00m message_list \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[32m    893\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m894\u001b[39m results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(\n\u001b[32m    895\u001b[39m     *[\n\u001b[32m    896\u001b[39m         \u001b[38;5;28mself\u001b[39m._agenerate_with_cache(\n\u001b[32m    897\u001b[39m             m,\n\u001b[32m    898\u001b[39m             stop=stop,\n\u001b[32m    899\u001b[39m             run_manager=run_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    900\u001b[39m             **kwargs,\n\u001b[32m    901\u001b[39m         )\n\u001b[32m    902\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages)\n\u001b[32m    903\u001b[39m     ],\n\u001b[32m    904\u001b[39m     return_exceptions=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    905\u001b[39m )\n\u001b[32m    906\u001b[39m exceptions = []\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, res \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1100\u001b[39m, in \u001b[36mBaseChatModel._agenerate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1098\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._agenerate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1100\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(\n\u001b[32m   1101\u001b[39m         messages, stop=stop, run_manager=run_manager, **kwargs\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1104\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._agenerate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:942\u001b[39m, in \u001b[36mChatOllama._agenerate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    935\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_agenerate\u001b[39m(\n\u001b[32m    936\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    937\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    940\u001b[39m     **kwargs: Any,\n\u001b[32m    941\u001b[39m ) -> ChatResult:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     final_chunk = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._achat_stream_with_aggregation(\n\u001b[32m    943\u001b[39m         messages, stop, run_manager, verbose=\u001b[38;5;28mself\u001b[39m.verbose, **kwargs\n\u001b[32m    944\u001b[39m     )\n\u001b[32m    945\u001b[39m     generation_info = final_chunk.generation_info\n\u001b[32m    946\u001b[39m     chat_generation = ChatGeneration(\n\u001b[32m    947\u001b[39m         message=AIMessage(\n\u001b[32m    948\u001b[39m             content=final_chunk.text,\n\u001b[32m   (...)\u001b[39m\u001b[32m    953\u001b[39m         generation_info=generation_info,\n\u001b[32m    954\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:761\u001b[39m, in \u001b[36mChatOllama._achat_stream_with_aggregation\u001b[39m\u001b[34m(self, messages, stop, run_manager, verbose, **kwargs)\u001b[39m\n\u001b[32m    752\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_achat_stream_with_aggregation\u001b[39m(\n\u001b[32m    753\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    754\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    758\u001b[39m     **kwargs: Any,\n\u001b[32m    759\u001b[39m ) -> ChatGenerationChunk:\n\u001b[32m    760\u001b[39m     final_chunk = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._aiterate_over_stream(messages, stop, **kwargs):\n\u001b[32m    762\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m final_chunk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    763\u001b[39m             final_chunk = chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:882\u001b[39m, in \u001b[36mChatOllama._aiterate_over_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_aiterate_over_stream\u001b[39m(\n\u001b[32m    876\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    877\u001b[39m     messages: \u001b[38;5;28mlist\u001b[39m[BaseMessage],\n\u001b[32m    878\u001b[39m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    879\u001b[39m     **kwargs: Any,\n\u001b[32m    880\u001b[39m ) -> AsyncIterator[ChatGenerationChunk]:\n\u001b[32m    881\u001b[39m     reasoning = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mreasoning\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m.reasoning)\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._acreate_chat_stream(messages, stop, **kwargs):\n\u001b[32m    883\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    884\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream_resp.get(\u001b[33m\"\u001b[39m\u001b[33mdone\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/langchain_ollama/chat_models.py:706\u001b[39m, in \u001b[36mChatOllama._acreate_chat_stream\u001b[39m\u001b[34m(self, messages, stop, **kwargs)\u001b[39m\n\u001b[32m    703\u001b[39m chat_params = \u001b[38;5;28mself\u001b[39m._chat_params(messages, stop, **kwargs)\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_params[\u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m706\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._async_client.chat(**chat_params):\n\u001b[32m    707\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    708\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/ollama/_client.py:684\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    681\u001b[39m   \u001b[38;5;28;01mawait\u001b[39;00m e.response.aread()\n\u001b[32m    682\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m ResponseError(e.response.text, e.response.status_code) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m684\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m r.aiter_lines():\n\u001b[32m    685\u001b[39m   part = json.loads(line)\n\u001b[32m    686\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m err := part.get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpx/_models.py:1031\u001b[39m, in \u001b[36mResponse.aiter_lines\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1029\u001b[39m decoder = LineDecoder()\n\u001b[32m   1030\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1031\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_text():\n\u001b[32m   1032\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m decoder.decode(text):\n\u001b[32m   1033\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m line\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpx/_models.py:1018\u001b[39m, in \u001b[36mResponse.aiter_text\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m   1016\u001b[39m chunker = TextChunker(chunk_size=chunk_size)\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1018\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m byte_content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_bytes():\n\u001b[32m   1019\u001b[39m         text_content = decoder.decode(byte_content)\n\u001b[32m   1020\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(text_content):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpx/_models.py:997\u001b[39m, in \u001b[36mResponse.aiter_bytes\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m    995\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m    996\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m--> \u001b[39m\u001b[32m997\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aiter_raw():\n\u001b[32m    998\u001b[39m         decoded = decoder.decode(raw_bytes)\n\u001b[32m    999\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(decoded):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpx/_models.py:1055\u001b[39m, in \u001b[36mResponse.aiter_raw\u001b[39m\u001b[34m(self, chunk_size)\u001b[39m\n\u001b[32m   1052\u001b[39m chunker = ByteChunker(chunk_size=chunk_size)\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=\u001b[38;5;28mself\u001b[39m._request):\n\u001b[32m-> \u001b[39m\u001b[32m1055\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m raw_stream_bytes \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stream:\n\u001b[32m   1056\u001b[39m         \u001b[38;5;28mself\u001b[39m._num_bytes_downloaded += \u001b[38;5;28mlen\u001b[39m(raw_stream_bytes)\n\u001b[32m   1057\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m chunker.decode(raw_stream_bytes):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpx/_client.py:176\u001b[39m, in \u001b[36mBoundAsyncStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    177\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m chunk\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpx/_transports/default.py:271\u001b[39m, in \u001b[36mAsyncResponseStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    270\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._httpcore_stream:\n\u001b[32m    272\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:407\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    405\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    406\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m407\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpcore/_async/connection_pool.py:403\u001b[39m, in \u001b[36mPoolByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__aiter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> typing.AsyncIterator[\u001b[38;5;28mbytes\u001b[39m]:\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream:\n\u001b[32m    404\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m part\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:342\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    340\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m AsyncShieldCancellation():\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.aclose()\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:334\u001b[39m, in \u001b[36mHTTP11ConnectionByteStream.__aiter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    333\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mreceive_response_body\u001b[39m\u001b[33m\"\u001b[39m, logger, \u001b[38;5;28mself\u001b[39m._request, kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m         \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection._receive_response_body(**kwargs):\n\u001b[32m    335\u001b[39m             \u001b[38;5;28;01myield\u001b[39;00m chunk\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    337\u001b[39m     \u001b[38;5;66;03m# If we get an exception while streaming the response,\u001b[39;00m\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# we want to close the response (and possibly the connection)\u001b[39;00m\n\u001b[32m    339\u001b[39m     \u001b[38;5;66;03m# before raising that exception.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:203\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_response_body\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    200\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     event = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._receive_event(timeout=timeout)\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Data):\n\u001b[32m    205\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mbytes\u001b[39m(event.data)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpcore/_async/http11.py:217\u001b[39m, in \u001b[36mAsyncHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_stream.read(\n\u001b[32m    218\u001b[39m         \u001b[38;5;28mself\u001b[39m.READ_NUM_BYTES, timeout=timeout\n\u001b[32m    219\u001b[39m     )\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/httpcore/_backends/anyio.py:35\u001b[39m, in \u001b[36mAnyIOStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m     34\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stream.receive(max_bytes=max_bytes)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m anyio.EndOfStream:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Personal Learning Folder/Personal Projects/OysterShell/.venv/lib/python3.12/site-packages/anyio/_backends/_asyncio.py:1254\u001b[39m, in \u001b[36mSocketStream.receive\u001b[39m\u001b[34m(self, max_bytes)\u001b[39m\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1249\u001b[39m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.is_set()\n\u001b[32m   1250\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._transport.is_closing()\n\u001b[32m   1251\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.is_at_eof\n\u001b[32m   1252\u001b[39m ):\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.resume_reading()\n\u001b[32m-> \u001b[39m\u001b[32m1254\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._protocol.read_event.wait()\n\u001b[32m   1255\u001b[39m     \u001b[38;5;28mself\u001b[39m._transport.pause_reading()\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/locks.py:212\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28mself\u001b[39m._waiters.append(fut)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m fut\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llamaindex/lib/python3.12/asyncio/futures.py:287\u001b[39m, in \u001b[36mFuture.__await__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    286\u001b[39m     \u001b[38;5;28mself\u001b[39m._asyncio_future_blocking = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m287\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m  \u001b[38;5;66;03m# This tells Task to wait for completion.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.done():\n\u001b[32m    289\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mawait wasn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt used with future\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "async for chunk in agent.astream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]},\n",
    "    stream_mode=\"updates\"\n",
    "):\n",
    "    # chunk['messages'].pretty_print()\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "089162d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='what is the weather in nyc?', additional_kwargs={}, response_metadata={}, id='72b6831f-29d8-46ac-84d7-bf1433521f7f'),\n",
       "  AIMessage(content=\"<think>\\nOkay, the user is asking for the weather in NYC. Let me check the tools available. There's a function called FunctionAgent that requires run_args. But the description is empty, so I'm not sure what it does. Maybe it's a placeholder or a generic function. Since there's no specific weather function listed, I can't call one to get the weather data. I should inform the user that I don't have the necessary tool to retrieve the weather information. I need to mention that the available tools don't include a weather function. Alright, time to respond clearly that I can't help with that request.\\n</think>\\n\\nThe available tools do not include a function for retrieving weather information. I cannot provide the current weather in NYC.\", additional_kwargs={}, response_metadata={'model': 'qwen3', 'created_at': '2025-07-16T14:42:25.923664Z', 'done': True, 'done_reason': 'stop', 'total_duration': 3236263125, 'load_duration': 40364667, 'prompt_eval_count': 145, 'prompt_eval_duration': 383952208, 'eval_count': 152, 'eval_duration': 2811508083, 'model_name': 'qwen3'}, id='run--3f435094-86b5-4e9d-9300-35945f6df55a-0', usage_metadata={'input_tokens': 145, 'output_tokens': 152, 'total_tokens': 297})]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d63d387",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oystershell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
