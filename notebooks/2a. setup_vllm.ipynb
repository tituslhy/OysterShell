{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNNR2zr8sNg8"
      },
      "source": [
        "Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA4ScRzIptu6",
        "outputId": "f963bf83-5248-4bb6-a402-80bca52230e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.4/383.4 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.0/169.0 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.6/87.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m792.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m110.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m102.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m87.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m343.3/343.3 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.2/96.2 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.0/15.0 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.2/44.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m734.6/734.6 kB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.6/856.6 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m385.5/385.5 kB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m453.1/453.1 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.4/42.4 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m127.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m912.7/912.7 kB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m950.5/950.5 kB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -qU pyngrok vllm huggingface-hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5we18lUauNGO"
      },
      "source": [
        "Check GPU utilization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nqaOx3UuOmk",
        "outputId": "36e95536-649f-483b-c9be-dd3bc637a542"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jul 19 15:19:32 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Meta's Llama models are gated on HuggingFace and require access. If you've acquired access from Meta on HuggingFace, you'll need to login to your HuggingFace account using your authorized token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "243faadb39c94f3b86bb77c3e7a614dc",
            "e2b783993094429aa156699fdc557f5d",
            "71c8f52c8c334bbabc580a08147964dc",
            "e058c856dfd346a38057496d797354d9",
            "92c830175ce9482fb5e712412e7c67db",
            "6b82bcba6ccf4cc3ac4909d59b72c95f",
            "2f7dda8ff4d14e77bcb9219c22221945",
            "3ac36d574304458cb85afa2cc803fcbc",
            "34b3c89fc91c4a988f1f30f5cd0f78e2",
            "4478273847ed430f8e7594339bbd9726",
            "c4a78a90aa3d452faca8b41999a56831",
            "558a79d69d55484689111ef29079c791",
            "e732f7beebe3467d879c264886797dd7",
            "fae94d9532504a9495ca8857bae942a0",
            "9e57121138ca462983eac57112e78fc0",
            "dc1cb10d23024c86b49062f28b649920",
            "6fbbd6be961247e9b1422ff46bae6155",
            "f440ee4b5995487289006a85e19a9920",
            "c513b168ae0b4e9b9a56c4031d4d7915",
            "4a4dc2dad1e545a1bc43c2d7377bf38c"
          ]
        },
        "id": "8qhnqxrvL8KS",
        "outputId": "91a39959-bdd5-40da-cfc1-4bf5c9566380"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "243faadb39c94f3b86bb77c3e7a614dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Serving our model on vLLM\n",
        "\n",
        "We start by pulling the chat template from vllm's github - because we want our Llama 3.2 LLM to be able to call functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDOdE7d-D_Ko",
        "outputId": "892c8cff-752f-41a0-ef95-3f6c4a5e1c5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2025-07-19 15:20:35--  https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/examples/tool_chat_template_llama3.1_json.jinja\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5255 (5.1K) [text/plain]\n",
            "Saving to: ‘tool_chat_template_llama3.1_json.jinja’\n",
            "\n",
            "tool_chat_template_ 100%[===================>]   5.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-07-19 15:20:35 (69.8 MB/s) - ‘tool_chat_template_llama3.1_json.jinja’ saved [5255/5255]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/examples/tool_chat_template_llama3.1_json.jinja"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2zR5lb3sRXC"
      },
      "source": [
        "We start by running `vllm serve <model>` using Python's subprocess library. Set `start_new_session=True` to allow code to continue to run on Jupyter notebooks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "OT1MyFc9rMyp"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "model = 'meta-llama/Llama-3.2-3B-Instruct'\n",
        "\n",
        "# Start vllm server in the background. The default hosting url is http://localhost:8000\n",
        "vllm_process = subprocess.Popen([\n",
        "    'vllm',\n",
        "    'serve',  # Subcommand must follow vllm\n",
        "    model,\n",
        "    '--enable-auto-tool-choice',\n",
        "    '--port', '8000',\n",
        "    '--tool-call-parser', 'llama3_json',\n",
        "    '--api-key', 'token-abc123',\n",
        "    '--chat-template', 'tool_chat_template_llama3.1_json.jinja',\n",
        "    '--trust-remote-code',\n",
        "    '--max-model-len', '16548', #reducing the max model length because it exceeded the KV cache limit\n",
        "], stdout=subprocess.PIPE, stderr=subprocess.PIPE, start_new_session=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ogTsGjEyskAe"
      },
      "source": [
        "We use the following utility function to be able to see the vLLM logs. If there is an error we want to print out the stderr messages for debugging.\n",
        "> As we are serving the vLLM model we are downloading the model directly from HuggingFace Hub so this can take awhile 😆"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "M8Ks1sn1rL8M"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import time\n",
        "from typing import Tuple\n",
        "import sys\n",
        "\n",
        "def check_vllm_status(url: str = \"http://localhost:8000/health\") -> bool:\n",
        "    \"\"\"Check if VLLM server is running and healthy.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        return response.status_code == 200\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        return False\n",
        "\n",
        "def monitor_vllm_process(vllm_process: subprocess.Popen, check_interval: int = 5) -> Tuple[bool, str, str]:\n",
        "    \"\"\"\n",
        "    Monitor VLLM process and return status, stdout, and stderr.\n",
        "    Returns: (success, stdout, stderr)\n",
        "    \"\"\"\n",
        "    print(\"Starting VLLM server monitoring...\")\n",
        "\n",
        "    while vllm_process.poll() is None:  # While process is still running\n",
        "        if check_vllm_status():\n",
        "            print(\"✓ VLLM server is up and running!\")\n",
        "            return True, \"\", \"\"\n",
        "\n",
        "        print(\"Waiting for VLLM server to start...\")\n",
        "        time.sleep(check_interval)\n",
        "\n",
        "        # Check if there's any output to display\n",
        "        if vllm_process.stdout.readable():\n",
        "            stdout = vllm_process.stdout.read1().decode('utf-8')\n",
        "            if stdout:\n",
        "                print(\"STDOUT:\", stdout)\n",
        "\n",
        "        if vllm_process.stderr.readable():\n",
        "            stderr = vllm_process.stderr.read1().decode('utf-8')\n",
        "            if stderr:\n",
        "                print(\"STDERR:\", stderr)\n",
        "\n",
        "    # If we get here, the process has ended\n",
        "    stdout, stderr = vllm_process.communicate()\n",
        "    return False, stdout.decode('utf-8'), stderr.decode('utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFQibLxNr9su",
        "outputId": "1c1f37c3-3d7e-474f-d53d-06c7496a4a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting VLLM server monitoring...\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 07-19 15:28:45 [__init__.py:244] Automatically detected platform cuda.\n",
            "\n",
            "STDERR: 2025-07-19 15:28:37.615035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752938917.635923    4554 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752938917.641959    4554 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-07-19 15:28:37.661978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 07-19 15:28:49 [api_server.py:1395] vLLM API server version 0.9.2\n",
            "INFO 07-19 15:28:49 [cli_args.py:325] non-default args: {'api_key': 'token-abc123', 'chat_template': 'tool_chat_template_llama3.1_json.jinja', 'enable_auto_tool_choice': True, 'tool_call_parser': 'llama3_json', 'model': 'meta-llama/Llama-3.2-3B-Instruct', 'trust_remote_code': True, 'max_model_len': 16548}\n",
            "\n",
            "STDERR: 2025-07-19 15:29:09.670138: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 07-19 15:29:04 [config.py:841] This model supports multiple tasks: {'classify', 'reward', 'generate', 'embed'}. Defaulting to 'generate'.\n",
            "WARNING 07-19 15:29:04 [config.py:3320] Your device 'Tesla T4' (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\n",
            "WARNING 07-19 15:29:04 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
            "INFO 07-19 15:29:04 [config.py:1472] Using max model len 16548\n",
            "WARNING 07-19 15:29:04 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "INFO 07-19 15:29:05 [api_server.py:268] Started engine process with PID 4712\n",
            "\n",
            "STDERR: WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1752938949.701577    4712 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1752938949.711053    4712 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 07-19 15:29:15 [__init__.py:244] Automatically detected platform cuda.\n",
            "INFO 07-19 15:29:18 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='meta-llama/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=16548, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=meta-llama/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":256,\"local_cache_dir\":null}, use_cached_outputs=True, \n",
            "INFO 07-19 15:29:19 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "INFO 07-19 15:29:19 [cuda.py:360] Using XFormers backend.\n",
            "\n",
            "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 07-19 15:29:20 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "INFO 07-19 15:29:20 [model_runner.py:1171] Starting to load model meta-llama/Llama-3.2-3B-Instruct...\n",
            "INFO 07-19 15:29:22 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
            "\n",
            "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:06<00:06,  6.33s/it]\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 07-19 15:29:50 [default_loader.py:272] Loading weights took 27.81 seconds\n",
            "\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:27<00:00, 15.17s/it]\n",
            "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:27<00:00, 13.85s/it]\n",
            "\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 07-19 15:29:51 [model_runner.py:1203] Model loading took 6.0160 GiB and 29.452817 seconds\n",
            "\n",
            "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 07-19 15:29:56 [worker.py:294] Memory profiling takes 4.44 seconds\n",
            "INFO 07-19 15:29:56 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\n",
            "INFO 07-19 15:29:56 [worker.py:294] model weights take 6.02GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 1.27GiB; the rest of the memory reserved for KV Cache is 5.94GiB.\n",
            "INFO 07-19 15:29:56 [executor_base.py:113] # cuda blocks: 3475, # CPU blocks: 2340\n",
            "INFO 07-19 15:29:56 [executor_base.py:118] Maximum concurrency for 16548 tokens per request: 3.36x\n",
            "INFO 07-19 15:30:01 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "\n",
            "Capturing CUDA graph shapes:   3%|▎         | 1/35 [00:02<01:28,  2.61s/it]\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 07-19 15:31:31 [model_runner.py:1671] Graph capturing finished in 90 secs, took 0.19 GiB\n",
            "\n",
            "Capturing CUDA graph shapes: 100%|██████████| 35/35 [01:29<00:00,  2.56s/it]\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: INFO 07-19 15:31:31 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 99.95 seconds\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] Using supplied chat template: {{- bos_token }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- if custom_tools is defined %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- set tools = custom_tools %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- if not tools_in_user_message is defined %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {#- Llama 3.1 doesn't pass all tests if the tools are in the system prompt #}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- set tools_in_user_message = true %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- if not date_string is defined %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- if strftime_now is defined %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- set date_string = strftime_now(\"%d %b %Y\") %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- else %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- set date_string = \"26 Jul 2024\" %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- if not tools is defined %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- set tools = none %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] \n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {#- This block extracts the system message, so we can slot it into the right place. #}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- if messages[0]['role'] == 'system' %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- if messages[0]['content'] is string %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- set system_message = messages[0]['content']|trim %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- else %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- set system_message = messages[0]['content'][0]['text']|trim %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- set messages = messages[1:] %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- else %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- if tools is not none %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- set system_message = \"You are a helpful assistant with tool calling capabilities. Only reply with a tool call if the function exists in the library provided by the user. If it doesn't exist, just reply directly in natural language. When you receive a tool call response, use the output to format an answer to the original user question.\" %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- else %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- set system_message = \"\" %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] \n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {#- System message #}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- if tools is not none %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {{- \"Environment: ipython\\n\" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- if tools is not none and not tools_in_user_message %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call. \" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {{- 'Respond in the format {\"name\": func\n",
            "STDERR: INFO:     Started server process [4554]\n",
            "INFO:     Waiting for application startup.\n",
            "\n",
            "Waiting for VLLM server to start...\n",
            "STDOUT: tion name, \"parameters\": dictionary of argument name and its value}. ' }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {{- \"Do not use variables.\\n\\n\" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- for t in tools %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {{- t | tojson(indent=4) }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {{- \"\\n\\n\" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- endfor %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {{- system_message }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {{- \"<|eot_id|>\" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] \n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {#- Custom tools are passed in a user message with some extra guidance #}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- if tools_in_user_message and not tools is none %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {#- Extract the first user message so we can plug it in here #}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- if messages | length != 0 %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- if messages[0]['content'] is string %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]             {%- set first_user_message = messages[0]['content']|trim %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- else %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]             {%- set first_user_message = messages[0]['content'] | selectattr('type', 'equalto', 'text') | map(attribute='text') | map('trim') | join('\\n') %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- set messages = messages[1:] %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- else %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}. ' }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {{- \"Do not use variables.\\n\\n\" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- for t in tools %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {{- t | tojson(indent=4) }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {{- \"\\n\\n\" }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- endfor %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {{- first_user_message + \"<|eot_id|>\"}}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] \n",
            "WARNING 07-19 15:31:35 [api_server.py:1261] {%- for message in messages %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]     {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n' }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- if message['content'] is string %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]             {{- message['content'] | trim}}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]         {%- else %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]             {%- for content in message['content'] %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]                 {%- if content['type'] == 'text' %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]                     {{- content['text'] | trim }}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]                 {%- endif %}\n",
            "WARNING 07-19 15:31:35 [api_server.py:1261]    \n",
            "STDERR: INFO:     Application startup complete.\n",
            "\n",
            "✓ VLLM server is up and running!\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    success, stdout, stderr = monitor_vllm_process(vllm_process)\n",
        "\n",
        "    if not success:\n",
        "        print(\"\\n❌ VLLM server failed to start!\")\n",
        "        print(\"\\nFull STDOUT:\", stdout)\n",
        "        print(\"\\nFull STDERR:\", stderr)\n",
        "        sys.exit(1)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\n⚠️ Monitoring interrupted by user\")\n",
        "    # # This should just exited the process of probing, not the vllm, if you want it then you coul uncomment this.\n",
        "    # vllm_process.terminate()\n",
        "    # try:\n",
        "    #     vllm_process.wait(timeout=5)\n",
        "    # except subprocess.TimeoutExpired:\n",
        "    #     vllm_process.kill()\n",
        "\n",
        "    stdout, stderr = vllm_process.communicate()\n",
        "    if stdout: print(\"\\nFinal STDOUT:\", stdout.decode('utf-8'))\n",
        "    if stderr: print(\"\\nFinal STDERR:\", stderr.decode('utf-8'))\n",
        "    sys.exit(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyeSS0PRu851",
        "outputId": "618b841b-296d-4dfc-8035-1a375f3bcc8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jul 19 15:32:15 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0             29W /   70W |   12652MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "dbtAY0t6qY2M"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "ngrok_auth_token = userdata.get('NGROK_AUTH_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0LdiDUpvW6t",
        "outputId": "fcebaf6f-32b5-4fc6-fb78-047837d064c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken {ngrok_auth_token}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6tmd9l_vAaj",
        "outputId": "08c04f63-85bc-4eaa-e079-8d5321c0bc2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * ngrok tunnel \"https://9398efd88ee3.ngrok-free.app\" -> \"http://127.0.0.1:8000\"\n"
          ]
        }
      ],
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "port = 8000\n",
        "# Open a ngrok tunnel to the HTTP server\n",
        "public_url = ngrok.connect(port).public_url\n",
        "print(f\" * ngrok tunnel \\\"{public_url}\\\" -> \\\"http://127.0.0.1:{port}\\\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fgozsc0vmWK"
      },
      "source": [
        "We have our free App!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lF5Hesaovjk1",
        "outputId": "0cbc41a4-36d4-44cd-b9d6-00c26baecb3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sat Jul 19 15:26:35 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P0             29W /   70W |   12784MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZXbAE7w1i83"
      },
      "source": [
        "# Kill process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30VtrMZcxaQI",
        "outputId": "8d1ead59-99b9-41aa-f6cf-0e0f0156f018"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vllm_process.terminate()\n",
        "vllm_process.wait()  # Wait for process to terminate"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
